{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading global-shark-attacks.zip to C:\\Users\\juanp\\Ironhack\\proyectos\\pandas-project\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/548k [00:00<?, ?B/s]\n",
      "100%|##########| 548k/548k [00:00<00:00, 5.39MB/s]\n",
      "100%|##########| 548k/548k [00:00<00:00, 5.34MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d teajay/global-shark-attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x attacks.csv\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf global-shark-attacks.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"attacks.csv\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Investigator or Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>25-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Boating</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Oceanside, San Diego County</td>\n",
       "      <td>Paddling</td>\n",
       "      <td>Julie Wolfe</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>White shark</td>\n",
       "      <td>R. Collier, GSAF</td>\n",
       "      <td>2018.06.25-Wolfe.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>18-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>St. Simon Island, Glynn County</td>\n",
       "      <td>Standing</td>\n",
       "      <td>Adyson McNeely</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.18-McNeely.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>6302.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>09-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Habush, Oahu</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>John Denges</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.09-Denges.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>6301.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>08-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Arrawarra Headland</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>B. Myatt, GSAF</td>\n",
       "      <td>2018.06.08-Arrawarra.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>04-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>La Ticla</td>\n",
       "      <td>Free diving</td>\n",
       "      <td>Gustavo Ramos</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark, 3m</td>\n",
       "      <td>A .Kipper</td>\n",
       "      <td>2018.06.04-Ramos.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>6299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Case Number         Date    Year        Type    Country             Area  \\\n",
       "0  2018.06.25  25-Jun-2018  2018.0     Boating        USA       California   \n",
       "1  2018.06.18  18-Jun-2018  2018.0  Unprovoked        USA          Georgia   \n",
       "2  2018.06.09  09-Jun-2018  2018.0     Invalid        USA           Hawaii   \n",
       "3  2018.06.08  08-Jun-2018  2018.0  Unprovoked  AUSTRALIA  New South Wales   \n",
       "4  2018.06.04  04-Jun-2018  2018.0    Provoked     MEXICO           Colima   \n",
       "\n",
       "                         Location     Activity             Name Sex   ...  \\\n",
       "0     Oceanside, San Diego County     Paddling      Julie Wolfe    F  ...   \n",
       "1  St. Simon Island, Glynn County     Standing  Adyson McNeely     F  ...   \n",
       "2                    Habush, Oahu      Surfing      John Denges    M  ...   \n",
       "3              Arrawarra Headland      Surfing             male    M  ...   \n",
       "4                        La Ticla  Free diving   Gustavo Ramos     M  ...   \n",
       "\n",
       "          Species           Investigator or Source                       pdf  \\\n",
       "0      White shark                R. Collier, GSAF      2018.06.25-Wolfe.pdf   \n",
       "1              NaN  K.McMurray, TrackingSharks.com    2018.06.18-McNeely.pdf   \n",
       "2              NaN  K.McMurray, TrackingSharks.com     2018.06.09-Denges.pdf   \n",
       "3        2 m shark                  B. Myatt, GSAF  2018.06.08-Arrawarra.pdf   \n",
       "4  Tiger shark, 3m                       A .Kipper      2018.06.04-Ramos.pdf   \n",
       "\n",
       "                                        href formula  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "\n",
       "                                                href Case Number.1  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.25   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.18   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.09   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.08   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.04   \n",
       "\n",
       "  Case Number.2 original order Unnamed: 22 Unnamed: 23  \n",
       "0    2018.06.25         6303.0         NaN         NaN  \n",
       "1    2018.06.18         6302.0         NaN         NaN  \n",
       "2    2018.06.09         6301.0         NaN         NaN  \n",
       "3    2018.06.08         6300.0         NaN         NaN  \n",
       "4    2018.06.04         6299.0         NaN         NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25723, 24)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
       "       'Unnamed: 23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a number of columns that wont be relevant for the analysis, like \"pdf\" of \"href formula\", and others whose meaning cannot be interpreted easily, like \"Unnamed 22\" or \"Original order\". This columsn can be dropped as they wont be useful. Also, the column \"Case Number\" and \"Date\" seem to contain the same date information with different formats, so we can delete one of them and work with the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"Date\",\"Name\",\"Investigator or Source\",\"pdf\",\"href formula\",\"href\",\"Case Number.1\",\"Case Number.2\",\n",
    "                 \"original order\",\"Unnamed: 22\", \"Unnamed: 23\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number    17021\n",
       "Year           19423\n",
       "Type           19425\n",
       "Country        19471\n",
       "Area           19876\n",
       "Location       19961\n",
       "Activity       19965\n",
       "Sex            19986\n",
       "Age            22252\n",
       "Injury         19449\n",
       "Fatal (Y/N)    19960\n",
       "Time           22775\n",
       "Species        22259\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a great number of rows that contain a lot of Nan values. An way to deal with them would be to drop all rows that have a 100% of NaN values, however, there are rows that have many NaN values without reaching the 100%. In order keep the maximun amount of information possible,the **thresh** parameter of the **dropna()** built-in function will be used to determine the maximum number of NaN values allowed per row. In this case the parameter will be set to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0, thresh=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6301, 13)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we have deleted around 19,000 rows of the dataset, almost 4/5 of the total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to treat the remaining NaN values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to fill the NaN values in a dataset, and each one depends on a series of factors like the type of the column, the kind of analysis, etc. In this case, two approaches are going to be taken. Those NaN values in string type columns are going to be replaced by **Unknown**, as it is something that should not be guessed nor inferred. In the case of data tyoe columns, even though they have not yet been transformed, are going to be repalced with the previous value because it can be assumed that they follow a certain order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When trying to replace the NaN values of categorical columns, there is an error related to the **Species** column, at is because it has a white space at the end. This can be solved later, but in order to have all the columns with the same format lets do it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"Species \":\"Species\"},inplace  =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the NaN with Unknown of categorical values (object)\n",
    "data[[\"Type\", \"Country\", \"Area\", \"Activity\", \"Injury\"\n",
    "            ,\"Sex \", \"Fatal (Y/N)\", \"Species\"]] = data[[\"Type\", \"Country\", \"Area\", \"Activity\", \"Injury\"\n",
    "                         ,\"Sex \", \"Fatal (Y/N)\",\"Species\"]].fillna(\"Unknown\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the case of 'Year' and 'Time' we're going to fill NaN values with the previous one, as we assume that they\n",
    "#follow a certain order, and doing this is better that filling them with 'NONE'\n",
    "\n",
    "data[[\"Case Number\",\"Time\", \"Year\"]] = data[[\"Case Number\",\"Time\", \"Year\"]].fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Column Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check each of the colums to see what corrections need to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(data):\n",
    "    return data.sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts(column):\n",
    "    '''\n",
    "    This function returns the value_counts() result of a column as a dictionary sorted by key in ascending order.\n",
    "    Easier to examine.\n",
    "    Args: the column of interest in the format of df.column or df[\"columnd\"]\n",
    "    '''\n",
    "    s = (column.value_counts())\n",
    "    dictionary =  s.to_dict()\n",
    "    return collections.OrderedDict(sorted(dictionary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, 125),\n",
       "             (5, 1),\n",
       "             (77, 1),\n",
       "             (500, 1),\n",
       "             (1543, 1),\n",
       "             (1554, 1),\n",
       "             (1555, 1),\n",
       "             (1580, 1),\n",
       "             (1595, 1),\n",
       "             (1617, 1),\n",
       "             (1637, 1),\n",
       "             (1642, 2),\n",
       "             (1700, 3),\n",
       "             (1703, 1),\n",
       "             (1721, 1),\n",
       "             (1723, 1),\n",
       "             (1733, 1),\n",
       "             (1738, 1),\n",
       "             (1742, 1),\n",
       "             (1748, 1),\n",
       "             (1749, 1),\n",
       "             (1751, 1),\n",
       "             (1753, 1),\n",
       "             (1755, 1),\n",
       "             (1758, 1),\n",
       "             (1764, 1),\n",
       "             (1767, 1),\n",
       "             (1771, 1),\n",
       "             (1776, 2),\n",
       "             (1779, 1),\n",
       "             (1780, 1),\n",
       "             (1783, 1),\n",
       "             (1784, 1),\n",
       "             (1785, 1),\n",
       "             (1786, 1),\n",
       "             (1787, 1),\n",
       "             (1788, 1),\n",
       "             (1791, 1),\n",
       "             (1792, 1),\n",
       "             (1797, 1),\n",
       "             (1800, 1),\n",
       "             (1801, 1),\n",
       "             (1802, 1),\n",
       "             (1803, 2),\n",
       "             (1804, 1),\n",
       "             (1805, 1),\n",
       "             (1807, 1),\n",
       "             (1808, 3),\n",
       "             (1810, 1),\n",
       "             (1811, 1),\n",
       "             (1812, 1),\n",
       "             (1815, 1),\n",
       "             (1816, 1),\n",
       "             (1817, 4),\n",
       "             (1818, 1),\n",
       "             (1819, 1),\n",
       "             (1822, 1),\n",
       "             (1823, 1),\n",
       "             (1825, 2),\n",
       "             (1826, 5),\n",
       "             (1827, 3),\n",
       "             (1828, 2),\n",
       "             (1829, 3),\n",
       "             (1830, 4),\n",
       "             (1831, 2),\n",
       "             (1832, 2),\n",
       "             (1834, 1),\n",
       "             (1835, 2),\n",
       "             (1836, 2),\n",
       "             (1837, 5),\n",
       "             (1839, 4),\n",
       "             (1840, 5),\n",
       "             (1841, 1),\n",
       "             (1842, 4),\n",
       "             (1844, 4),\n",
       "             (1845, 6),\n",
       "             (1846, 3),\n",
       "             (1847, 9),\n",
       "             (1848, 4),\n",
       "             (1849, 7),\n",
       "             (1850, 2),\n",
       "             (1851, 3),\n",
       "             (1852, 10),\n",
       "             (1853, 8),\n",
       "             (1855, 7),\n",
       "             (1856, 4),\n",
       "             (1857, 1),\n",
       "             (1858, 6),\n",
       "             (1859, 1),\n",
       "             (1860, 7),\n",
       "             (1861, 5),\n",
       "             (1862, 12),\n",
       "             (1863, 11),\n",
       "             (1864, 14),\n",
       "             (1865, 4),\n",
       "             (1866, 2),\n",
       "             (1867, 5),\n",
       "             (1868, 6),\n",
       "             (1869, 2),\n",
       "             (1870, 8),\n",
       "             (1871, 9),\n",
       "             (1872, 8),\n",
       "             (1873, 4),\n",
       "             (1874, 12),\n",
       "             (1875, 5),\n",
       "             (1876, 9),\n",
       "             (1877, 14),\n",
       "             (1878, 13),\n",
       "             (1879, 8),\n",
       "             (1880, 15),\n",
       "             (1881, 9),\n",
       "             (1882, 12),\n",
       "             (1883, 13),\n",
       "             (1884, 7),\n",
       "             (1885, 9),\n",
       "             (1886, 15),\n",
       "             (1887, 13),\n",
       "             (1888, 15),\n",
       "             (1889, 9),\n",
       "             (1890, 17),\n",
       "             (1891, 9),\n",
       "             (1892, 11),\n",
       "             (1893, 13),\n",
       "             (1894, 15),\n",
       "             (1895, 20),\n",
       "             (1896, 12),\n",
       "             (1897, 11),\n",
       "             (1898, 21),\n",
       "             (1899, 20),\n",
       "             (1900, 13),\n",
       "             (1901, 9),\n",
       "             (1902, 15),\n",
       "             (1903, 10),\n",
       "             (1904, 14),\n",
       "             (1905, 18),\n",
       "             (1906, 21),\n",
       "             (1907, 23),\n",
       "             (1908, 13),\n",
       "             (1909, 18),\n",
       "             (1910, 14),\n",
       "             (1911, 16),\n",
       "             (1912, 14),\n",
       "             (1913, 18),\n",
       "             (1914, 18),\n",
       "             (1915, 12),\n",
       "             (1916, 25),\n",
       "             (1917, 11),\n",
       "             (1918, 5),\n",
       "             (1919, 14),\n",
       "             (1920, 15),\n",
       "             (1921, 12),\n",
       "             (1922, 22),\n",
       "             (1923, 21),\n",
       "             (1924, 19),\n",
       "             (1925, 14),\n",
       "             (1926, 21),\n",
       "             (1927, 19),\n",
       "             (1928, 26),\n",
       "             (1929, 38),\n",
       "             (1930, 26),\n",
       "             (1931, 29),\n",
       "             (1932, 27),\n",
       "             (1933, 22),\n",
       "             (1934, 27),\n",
       "             (1935, 32),\n",
       "             (1936, 32),\n",
       "             (1937, 30),\n",
       "             (1938, 24),\n",
       "             (1939, 25),\n",
       "             (1940, 24),\n",
       "             (1941, 27),\n",
       "             (1942, 41),\n",
       "             (1943, 28),\n",
       "             (1944, 31),\n",
       "             (1945, 16),\n",
       "             (1946, 26),\n",
       "             (1947, 30),\n",
       "             (1948, 29),\n",
       "             (1949, 31),\n",
       "             (1950, 43),\n",
       "             (1951, 32),\n",
       "             (1952, 29),\n",
       "             (1953, 36),\n",
       "             (1954, 42),\n",
       "             (1955, 43),\n",
       "             (1956, 51),\n",
       "             (1957, 41),\n",
       "             (1958, 54),\n",
       "             (1959, 93),\n",
       "             (1960, 93),\n",
       "             (1961, 78),\n",
       "             (1962, 86),\n",
       "             (1963, 61),\n",
       "             (1964, 66),\n",
       "             (1965, 51),\n",
       "             (1966, 58),\n",
       "             (1967, 48),\n",
       "             (1968, 47),\n",
       "             (1969, 30),\n",
       "             (1970, 42),\n",
       "             (1971, 28),\n",
       "             (1972, 35),\n",
       "             (1973, 27),\n",
       "             (1974, 38),\n",
       "             (1975, 49),\n",
       "             (1976, 39),\n",
       "             (1977, 26),\n",
       "             (1978, 25),\n",
       "             (1979, 25),\n",
       "             (1980, 35),\n",
       "             (1981, 49),\n",
       "             (1982, 40),\n",
       "             (1983, 50),\n",
       "             (1984, 41),\n",
       "             (1985, 37),\n",
       "             (1986, 39),\n",
       "             (1987, 35),\n",
       "             (1988, 55),\n",
       "             (1989, 53),\n",
       "             (1990, 38),\n",
       "             (1991, 38),\n",
       "             (1992, 56),\n",
       "             (1993, 56),\n",
       "             (1994, 56),\n",
       "             (1995, 76),\n",
       "             (1996, 61),\n",
       "             (1997, 57),\n",
       "             (1998, 65),\n",
       "             (1999, 66),\n",
       "             (2000, 97),\n",
       "             (2001, 92),\n",
       "             (2002, 88),\n",
       "             (2003, 92),\n",
       "             (2004, 92),\n",
       "             (2005, 103),\n",
       "             (2006, 103),\n",
       "             (2007, 112),\n",
       "             (2008, 122),\n",
       "             (2009, 120),\n",
       "             (2010, 101),\n",
       "             (2011, 128),\n",
       "             (2012, 117),\n",
       "             (2013, 122),\n",
       "             (2014, 127),\n",
       "             (2015, 143),\n",
       "             (2016, 130),\n",
       "             (2017, 137),\n",
       "             (2018, 53)])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts(data.Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that should be noticed is that ther a 125 attack in year 0, which makes no sense. Also, there are records of attacks taking place in years that are too ancient. This may be typing errors, so, instead of deleting all those records, and losing all the information, when analyzing the **Year** column a threshold will be set to avoid taking this attypical years into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be mentioned that the years have decimals, which is not possible. Therefore, lets remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Year = data.Year.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The df has been safed as a csv so that the progress is not lost if jupyter notebook is closed \n",
    "data.to_csv(r'C:\\Users\\juanp\\Ironhack\\proyectos\\pandas-project\\data\\data_year.csv' , index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = pd.read_csv(\"data\\data_year.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Country**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(' PHILIPPINES', 1),\n",
       "             (' TONGA', 3),\n",
       "             ('ADMIRALTY ISLANDS', 1),\n",
       "             ('AFRICA', 1),\n",
       "             ('ALGERIA', 1),\n",
       "             ('AMERICAN SAMOA', 3),\n",
       "             ('ANDAMAN / NICOBAR ISLANDAS', 1),\n",
       "             ('ANDAMAN ISLANDS', 1),\n",
       "             ('ANGOLA', 1),\n",
       "             ('ANTIGUA', 2),\n",
       "             ('ARGENTINA', 1),\n",
       "             ('ARUBA', 1),\n",
       "             ('ASIA?', 1),\n",
       "             ('ATLANTIC OCEAN', 17),\n",
       "             ('AUSTRALIA', 1338),\n",
       "             ('AZORES', 5),\n",
       "             ('BAHAMAS', 109),\n",
       "             ('BAHREIN', 1),\n",
       "             ('BANGLADESH', 1),\n",
       "             ('BARBADOS', 6),\n",
       "             ('BAY OF BENGAL', 1),\n",
       "             ('BELIZE', 3),\n",
       "             ('BERMUDA', 16),\n",
       "             ('BRAZIL', 112),\n",
       "             ('BRITISH ISLES', 1),\n",
       "             ('BRITISH NEW GUINEA', 1),\n",
       "             ('BRITISH VIRGIN ISLANDS', 1),\n",
       "             ('BRITISH WEST INDIES', 1),\n",
       "             ('BURMA', 4),\n",
       "             ('Between PORTUGAL & INDIA', 1),\n",
       "             ('CANADA', 12),\n",
       "             ('CAPE VERDE', 3),\n",
       "             ('CARIBBEAN SEA', 8),\n",
       "             ('CAYMAN ISLANDS', 2),\n",
       "             ('CENTRAL PACIFIC', 2),\n",
       "             ('CEYLON', 3),\n",
       "             ('CEYLON (SRI LANKA)', 1),\n",
       "             ('CHILE', 8),\n",
       "             ('CHINA', 7),\n",
       "             ('COLUMBIA', 9),\n",
       "             ('COMOROS', 1),\n",
       "             ('COOK ISLANDS', 1),\n",
       "             ('COSTA RICA', 17),\n",
       "             ('CRETE', 2),\n",
       "             ('CROATIA', 34),\n",
       "             ('CUBA', 46),\n",
       "             ('CURACAO', 1),\n",
       "             ('CYPRUS', 1),\n",
       "             ('Coast of AFRICA', 1),\n",
       "             ('DIEGO GARCIA', 1),\n",
       "             ('DJIBOUTI', 1),\n",
       "             ('DOMINICAN REPUBLIC', 7),\n",
       "             ('ECUADOR', 9),\n",
       "             ('EGYPT', 38),\n",
       "             ('EGYPT ', 2),\n",
       "             ('EGYPT / ISRAEL', 1),\n",
       "             ('EL SALVADOR', 4),\n",
       "             ('ENGLAND', 23),\n",
       "             ('EQUATORIAL GUINEA / CAMEROON', 1),\n",
       "             ('FALKLAND ISLANDS', 1),\n",
       "             ('FEDERATED STATES OF MICRONESIA', 1),\n",
       "             ('FIJI', 62),\n",
       "             ('FRANCE', 13),\n",
       "             ('FRENCH POLYNESIA', 25),\n",
       "             ('Fiji', 3),\n",
       "             ('GABON', 1),\n",
       "             ('GEORGIA', 1),\n",
       "             ('GHANA', 1),\n",
       "             ('GRAND CAYMAN', 1),\n",
       "             ('GREECE', 25),\n",
       "             ('GREENLAND', 1),\n",
       "             ('GRENADA', 4),\n",
       "             ('GUAM', 4),\n",
       "             ('GUATEMALA', 1),\n",
       "             ('GUINEA', 3),\n",
       "             ('GULF OF ADEN', 1),\n",
       "             ('GUYANA', 3),\n",
       "             ('HAITI', 3),\n",
       "             ('HONDURAS', 4),\n",
       "             ('HONG KONG', 24),\n",
       "             ('ICELAND', 2),\n",
       "             ('INDIA', 40),\n",
       "             ('INDIAN OCEAN', 7),\n",
       "             ('INDIAN OCEAN?', 1),\n",
       "             ('INDONESIA', 23),\n",
       "             ('IRAN', 29),\n",
       "             ('IRAN / IRAQ', 1),\n",
       "             ('IRAQ', 12),\n",
       "             ('IRELAND', 1),\n",
       "             ('ISRAEL', 7),\n",
       "             ('ITALY', 71),\n",
       "             ('ITALY / CROATIA', 1),\n",
       "             ('JAMAICA', 27),\n",
       "             ('JAPAN', 34),\n",
       "             ('JAVA', 1),\n",
       "             ('JOHNSTON ISLAND', 2),\n",
       "             ('KENYA', 10),\n",
       "             ('KIRIBATI', 6),\n",
       "             ('KOREA', 1),\n",
       "             ('KUWAIT', 1),\n",
       "             ('LEBANON', 3),\n",
       "             ('LIBERIA', 3),\n",
       "             ('LIBYA', 6),\n",
       "             ('MADAGASCAR', 8),\n",
       "             ('MALAYSIA', 5),\n",
       "             ('MALDIVE ISLANDS', 1),\n",
       "             ('MALDIVES', 1),\n",
       "             ('MALTA', 5),\n",
       "             ('MARSHALL ISLANDS', 13),\n",
       "             ('MARTINIQUE', 3),\n",
       "             ('MAURITIUS', 10),\n",
       "             ('MAYOTTE', 1),\n",
       "             ('MEDITERRANEAN SEA', 2),\n",
       "             ('MEXICO', 89),\n",
       "             ('MEXICO ', 1),\n",
       "             ('MICRONESIA', 3),\n",
       "             ('MID ATLANTIC OCEAN', 5),\n",
       "             ('MID-PACIFC OCEAN', 1),\n",
       "             ('MONACO', 1),\n",
       "             ('MONTENEGRO', 3),\n",
       "             ('MOZAMBIQUE', 45),\n",
       "             ('NAMIBIA', 2),\n",
       "             ('NETHERLANDS ANTILLES', 1),\n",
       "             ('NEVIS', 1),\n",
       "             ('NEW BRITAIN', 6),\n",
       "             ('NEW CALEDONIA', 53),\n",
       "             ('NEW GUINEA', 10),\n",
       "             ('NEW ZEALAND', 128),\n",
       "             ('NICARAGUA', 6),\n",
       "             ('NICARAGUA ', 1),\n",
       "             ('NIGERIA', 4),\n",
       "             ('NORTH ATLANTIC OCEAN', 4),\n",
       "             ('NORTH ATLANTIC OCEAN ', 1),\n",
       "             ('NORTH PACIFIC OCEAN', 7),\n",
       "             ('NORTH SEA', 1),\n",
       "             ('NORTHERN ARABIAN SEA', 1),\n",
       "             ('NORTHERN MARIANA ISLANDS', 1),\n",
       "             ('NORWAY', 2),\n",
       "             ('OCEAN', 1),\n",
       "             ('OKINAWA', 6),\n",
       "             ('PACIFIC OCEAN', 17),\n",
       "             ('PACIFIC OCEAN ', 2),\n",
       "             ('PALAU', 5),\n",
       "             ('PALESTINIAN TERRITORIES', 1),\n",
       "             ('PANAMA', 32),\n",
       "             ('PAPUA NEW GUINEA', 134),\n",
       "             ('PARAGUAY', 1),\n",
       "             ('PERSIAN GULF', 4),\n",
       "             ('PERU', 1),\n",
       "             ('PHILIPPINES', 61),\n",
       "             ('PORTUGAL', 3),\n",
       "             ('PUERTO RICO', 1),\n",
       "             ('RED SEA', 1),\n",
       "             ('RED SEA / INDIAN OCEAN', 1),\n",
       "             ('RED SEA?', 1),\n",
       "             ('REUNION', 60),\n",
       "             ('REUNION ISLAND', 1),\n",
       "             ('ROATAN', 1),\n",
       "             ('RUSSIA', 4),\n",
       "             ('SAMOA', 8),\n",
       "             ('SAN DOMINGO', 1),\n",
       "             ('SAUDI ARABIA', 5),\n",
       "             ('SCOTLAND', 8),\n",
       "             ('SENEGAL', 11),\n",
       "             ('SEYCHELLES', 7),\n",
       "             ('SIERRA LEONE', 8),\n",
       "             ('SINGAPORE', 6),\n",
       "             ('SLOVENIA', 1),\n",
       "             ('SOLOMON ISLANDS', 30),\n",
       "             ('SOLOMON ISLANDS / VANUATU', 1),\n",
       "             ('SOMALIA', 6),\n",
       "             ('SOUTH AFRICA', 579),\n",
       "             ('SOUTH ATLANTIC OCEAN', 12),\n",
       "             ('SOUTH CHINA SEA', 1),\n",
       "             ('SOUTH KOREA', 8),\n",
       "             ('SOUTH PACIFIC OCEAN', 2),\n",
       "             ('SOUTHWEST PACIFIC OCEAN', 2),\n",
       "             ('SPAIN', 44),\n",
       "             ('SRI LANKA', 14),\n",
       "             ('ST HELENA, British overseas territory', 2),\n",
       "             ('ST. MAARTIN', 1),\n",
       "             ('ST. MARTIN', 1),\n",
       "             ('SUDAN', 4),\n",
       "             ('SUDAN?', 1),\n",
       "             ('SWEDEN', 1),\n",
       "             ('SYRIA', 1),\n",
       "             ('Seychelles', 1),\n",
       "             ('Sierra Leone', 1),\n",
       "             ('TAIWAN', 9),\n",
       "             ('TANZANIA', 8),\n",
       "             ('TASMAN SEA', 1),\n",
       "             ('THAILAND', 8),\n",
       "             ('THE BALKANS', 1),\n",
       "             ('TOBAGO', 2),\n",
       "             ('TONGA', 15),\n",
       "             ('TRINIDAD & TOBAGO', 3),\n",
       "             ('TUNISIA', 3),\n",
       "             ('TURKEY', 12),\n",
       "             ('TURKS & CAICOS', 5),\n",
       "             ('TUVALU', 1),\n",
       "             ('UNITED ARAB EMIRATES', 2),\n",
       "             ('UNITED ARAB EMIRATES (UAE)', 2),\n",
       "             ('UNITED KINGDOM', 11),\n",
       "             ('URUGUAY', 4),\n",
       "             ('USA', 2229),\n",
       "             ('Unknown', 49),\n",
       "             ('VANUATU', 14),\n",
       "             ('VENEZUELA', 11),\n",
       "             ('VIETNAM', 15),\n",
       "             ('WEST INDIES', 2),\n",
       "             ('WESTERN SAMOA', 1),\n",
       "             ('YEMEN', 2),\n",
       "             ('YEMEN ', 7)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts(data.Country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Country** column is a huge mess. There are countries that are instanecs that have the same name but one has an extra space at the end, some have two possible locations, etc. The first thing that must be done is to make sure all names have the same capitalization, in this case uppercased, and delete all extra spaces. Afterwards, the a function will be created to deal with the different pattern cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete whitespace at the end and beggining of string\n",
    "data.Country = data.Country.str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Country = data.Country.str.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uppercase\n",
    "data.Country = data.Country.apply(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_character(column, pattern, substitute):\n",
    "    column.replace(to_replace = pattern, value = substitute, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-e98904090171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprueba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCountry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprueba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Country'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace_character\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprueba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCountry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'B'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mr'A'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\varios\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4106\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4107\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4108\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "prueba.Country = prueba['Country'].apply(replace_character(prueba.Country, r'B',r'A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change CEYLON (SRI LANKA) for SRI LANKA\n",
    "\n",
    "data_clean.Country = data_clean.Country.replace(to_replace = r'\\s\\(\\w{3}\\s\\w{5}\\)' , value = \"\", regex = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Country = data_clean.Country.replace(to_replace = \"CEYLON\" , value = \"SRI LANKA\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminat e the '?' at the end of certain countries'\n",
    "\n",
    "data_clean.Country = data_clean.Country.replace(to_replace = r\"\\?$\", value = \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace ST MAARTIN for ST MARTIN\n",
    "\n",
    "prueba.Country = prueba.Country.replace(to_replace = 'MAARTIN', value = 'MARTIN', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Country = data_clean.Country.replace(to_replace = 'U[A-Z][A-Z][A-Z][A-Z][A-Z][A-Z]', value = 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_clean.Country.sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                 2229\n",
       "AUSTRALIA           1338\n",
       "SOUTH AFRICA         579\n",
       "PAPUA NEW GUINEA     134\n",
       "NEW ZEALAND          128\n",
       "BRAZIL               112\n",
       "BAHAMAS              109\n",
       "MEXICO                89\n",
       "ITALY                 71\n",
       "FIJI                  62\n",
       "PHILIPPINES           62\n",
       "REUNION               60\n",
       "NEW CALEDONIA         53\n",
       "Unknown               50\n",
       "CUBA                  46\n",
       "MOZAMBIQUE            45\n",
       "SPAIN                 44\n",
       "INDIA                 40\n",
       "EGYPT                 38\n",
       "JAPAN                 34\n",
       "Name: Country, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.Country.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the 'Time' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working between ranges is not ging to be helpful, so we are going to use the lower hour for this analysis\n",
    "#first, we delete the section that goes after '--''\n",
    "\n",
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(s\\--\\s\\w{2}\\w\\w{2})', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afternoon is the time between noon and evening, so we are going to assing 16h00 as the midpoint\n",
    "\n",
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Afternoon', value ='16h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dusk starts around 18:30\n",
    "\n",
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Dusk', value ='18h30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Early morning', value ='07h30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Early afternoon', value ='18h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Evening', value ='20h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Late afternoon', value ='19h30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Midday', value ='12h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Midnight', value ='00h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Morning', value ='08h30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = 'Night', value ='21:30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now those that only have one '-'\n",
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(s\\-\\s\\w{2}\\w\\w{2})', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'\\s*', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'\\s$', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\/\\s\\w{5})', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\s\\-\\w{2}\\w\\w{2})', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\-\\w{2}\\w\\w{2})', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'( \\w{2} \\w{5})', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\/\\s\\w{5})', value = '', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\,\\s\\w{4})', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'\\,\\w', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\s\\(\\w*\\))', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\(\\w*\\))', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'( \\-\\w*\\s\\w*\\s\\w*\\s\\w*)', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'[a-z]{2}\\w*', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\/\\w*)', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\<*)', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\>*)', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'(\\?$)', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = '13h345', value ='13h45', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = '15h00j', value ='15h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = '0830', value ='08:30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = '11h115', value ='11h15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = '1500', value ='15h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = '06j00', value ='06h00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Time = data_clean.Time.replace(to_replace = r'[a-z]', value =':', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_clean.Time.sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the 'Type' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Type = data_clean.Type.replace(to_replace = 'Boating', value = 'Boat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Type = data_clean.Type.replace(to_replace = 'Boatomg', value = 'Boat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unprovoked      4595\n",
       "Provoked         574\n",
       "Invalid          547\n",
       "Boat             341\n",
       "Sea Disaster     239\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.Type.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the 'Activity' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm not interested in the subdivisions of the activties, so I'm going to eleiminate all the unnecesary information\n",
    "\n",
    "data_clean.Activity= data_clean.Activity.replace(to_replace = r'\\s(\\w*\\s)*\\w*', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete all parentheis after the first word\n",
    "\n",
    "data_clean.Activity= data_clean.Activity.replace(to_replace = r'(\\-\\w*)+', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting all non word characters\n",
    "\n",
    "data_clean.Activity= data_clean.Activity.replace(to_replace = r'\\W+', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Surf', value ='Surfing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Body', value ='Surfing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Surfinging', value ='Surfing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Surfingsitting', value ='Surfing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Windsurfing', value ='Surfing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Paddling', value ='Surfing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Paddle', value ='Surfing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Kayak', value ='Kayaking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Kayakinging', value ='Kayaking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Freedom', value ='Swimming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Scuba', value ='Diving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Dived', value ='Diving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Divingsubmerged', value ='Diving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Divingdiving', value ='Diving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Divingbut', value ='Diving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Pearl', value ='Diving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Free', value ='Diving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Floating', value ='Bathing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'boat', value ='Boating' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Boat', value ='Boating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Boatinging', value ='Boating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.Activity= data_clean.Activity.replace(to_replace = 'Sailing', value ='Boating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_clean.Activity.sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Surfing         1233\n",
       "Swimming        1073\n",
       "Fishing          599\n",
       "Unknown          548\n",
       "Diving           463\n",
       "Spearfishing     395\n",
       "Bathing          227\n",
       "Wading           164\n",
       "Standing         130\n",
       "Snorkeling        93\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.Activity.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the 'Sex' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_clean[\"Sex \"].sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Sex \"]= data_clean[\"Sex \"].replace(to_replace = 'N', value ='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Sex \"]= data_clean[\"Sex \"].replace(to_replace = 'lli', value ='Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Sex \"]= data_clean[\"Sex \"].replace(to_replace = '.', value ='Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the 'Fatal' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Fatal (Y/N)\"] = data_clean[\"Fatal (Y/N)\"].replace(to_replace = r'\\s*', value ='', regex = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Fatal (Y/N)\"] = data_clean[\"Fatal (Y/N)\"].replace(to_replace = 'M', value = 'N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Fatal (Y/N)\"] = data_clean[\"Fatal (Y/N)\"].replace(to_replace = 'y', value = 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Fatal (Y/N)\"] = data_clean[\"Fatal (Y/N)\"].replace(to_replace = '2017', value = 'Unknown' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Fatal (Y/N)\"] = data_clean[\"Fatal (Y/N)\"].replace(to_replace = 'UNKNOWN', value = 'Unknown' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_clean[\"Fatal (Y/N)\"].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know they don't have the exact same information, but, as the number of observations that differ is very small, we can use just one of them. In this case we're going to use the 'Case Number' column, as we can assume that it is the combination of 'Case Number.1' and 'Case NUmber.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean .drop([\"Case Number.1\", \"Case Number.2\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we are going to clean the 'Case Number' column in order to have just one format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We delele the fragemnts that come after the last '.'\n",
    "\n",
    "data_clean[\"Case Number\"] = data_clean[\"Case Number\"].replace(to_replace = r'(\\.\\w$)', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We delete the combination of uppercase and number after a '.'\n",
    "\n",
    "data_clean[\"Case Number\"] = data_clean[\"Case Number\"].replace(to_replace = r'\\.[A-Z]\\w', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We delete the combination of upper or lowercase that come after a '.'\n",
    "\n",
    "data_clean[\"Case Number\"] = data_clean[\"Case Number\"].replace(to_replace = r'\\.[A-Z]|[a-z]', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We delete the last '.' \n",
    "\n",
    "data_clean[\"Case Number\"] = data_clean[\"Case Number\"].replace(to_replace = r'\\.$', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We delete the '.' and '&' combination\n",
    "\n",
    "data_clean[\"Case Number\"] = data_clean[\"Case Number\"].replace(to_replace = r'\\.\\ \\&', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We delete the '/'\n",
    "\n",
    "data_clean[\"Case Number\"] = data_clean[\"Case Number\"].replace(to_replace = r'\\/', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We delete the combination of '.' or '-' and 0-1 letter after\n",
    "\n",
    "data_clean[\"Case Number\"] = data_clean[\"Case Number\"].replace(to_replace = r'(\\.|-)\\w?$', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original order</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6303.0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Boat</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>F</td>\n",
       "      <td>No injury to occupant, outrigger canoe and pad...</td>\n",
       "      <td>N</td>\n",
       "      <td>18h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302.0</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Standing</td>\n",
       "      <td>F</td>\n",
       "      <td>Minor injury to left thigh</td>\n",
       "      <td>N</td>\n",
       "      <td>14h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301.0</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Surfinging</td>\n",
       "      <td>M</td>\n",
       "      <td>Injury to left lower leg from surfboard skeg</td>\n",
       "      <td>N</td>\n",
       "      <td>07h45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300.0</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Surfinging</td>\n",
       "      <td>M</td>\n",
       "      <td>Minor injury to lower leg</td>\n",
       "      <td>N</td>\n",
       "      <td>07h45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299.0</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>Diving</td>\n",
       "      <td>M</td>\n",
       "      <td>Lacerations to leg &amp; hand shark PROVOKED INCIDENT</td>\n",
       "      <td>N</td>\n",
       "      <td>07h45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>ND.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>Diving</td>\n",
       "      <td>M</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>Y</td>\n",
       "      <td>16h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>ND.0004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>Diving</td>\n",
       "      <td>M</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>Y</td>\n",
       "      <td>16h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>ND.0003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>M</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>Y</td>\n",
       "      <td>16h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>ND.0002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>M</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>Y</td>\n",
       "      <td>16h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ND.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>SRI LANKA</td>\n",
       "      <td>Eastern Province</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>M</td>\n",
       "      <td>FATAL. \"Shark bit him in half, carrying away t...</td>\n",
       "      <td>Y</td>\n",
       "      <td>16h00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6302 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Case Number    Year        Type    Country               Area  \\\n",
       "original order                                                                 \n",
       "6303.0          2018.06.25  2018.0        Boat        USA         California   \n",
       "6302.0          2018.06.18  2018.0  Unprovoked        USA            Georgia   \n",
       "6301.0          2018.06.09  2018.0     Invalid        USA             Hawaii   \n",
       "6300.0          2018.06.08  2018.0  Unprovoked  AUSTRALIA    New South Wales   \n",
       "6299.0          2018.06.04  2018.0    Provoked     MEXICO             Colima   \n",
       "...                    ...     ...         ...        ...                ...   \n",
       "6.0                ND.0005     0.0  Unprovoked  AUSTRALIA  Western Australia   \n",
       "5.0                ND.0004     0.0  Unprovoked  AUSTRALIA  Western Australia   \n",
       "4.0                ND.0003     0.0  Unprovoked        USA     North Carolina   \n",
       "3.0                ND.0002     0.0  Unprovoked     PANAMA            Unknown   \n",
       "2.0                ND.0001     0.0  Unprovoked  SRI LANKA   Eastern Province   \n",
       "\n",
       "                  Activity Sex   \\\n",
       "original order                    \n",
       "6303.0             Surfing    F   \n",
       "6302.0            Standing    F   \n",
       "6301.0          Surfinging    M   \n",
       "6300.0          Surfinging    M   \n",
       "6299.0              Diving    M   \n",
       "...                    ...  ...   \n",
       "6.0                 Diving    M   \n",
       "5.0                 Diving    M   \n",
       "4.0               Swimming    M   \n",
       "3.0                Unknown    M   \n",
       "2.0               Swimming    M   \n",
       "\n",
       "                                                           Injury Fatal (Y/N)  \\\n",
       "original order                                                                  \n",
       "6303.0          No injury to occupant, outrigger canoe and pad...           N   \n",
       "6302.0                                 Minor injury to left thigh           N   \n",
       "6301.0               Injury to left lower leg from surfboard skeg           N   \n",
       "6300.0                                  Minor injury to lower leg           N   \n",
       "6299.0          Lacerations to leg & hand shark PROVOKED INCIDENT           N   \n",
       "...                                                           ...         ...   \n",
       "6.0                                                         FATAL           Y   \n",
       "5.0                                                         FATAL           Y   \n",
       "4.0                                                         FATAL           Y   \n",
       "3.0                                                         FATAL           Y   \n",
       "2.0             FATAL. \"Shark bit him in half, carrying away t...           Y   \n",
       "\n",
       "                 Time  \n",
       "original order         \n",
       "6303.0          18h00  \n",
       "6302.0          14h00  \n",
       "6301.0          07h45  \n",
       "6300.0          07h45  \n",
       "6299.0          07h45  \n",
       "...               ...  \n",
       "6.0             16h00  \n",
       "5.0             16h00  \n",
       "4.0             16h00  \n",
       "3.0             16h00  \n",
       "2.0             16h00  \n",
       "\n",
       "[6302 rows x 10 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We export the clean dataframe as csv.\n",
    "\n",
    "data_clean.to_csv(r'C:\\Users\\juanp\\Ironhack\\pandas-project\\data\\data_clean.csv' , index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
